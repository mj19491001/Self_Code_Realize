&#8195;&#8195;时隔好久，这次给大家带来神经网络模型的介绍。
# 神经网络模型简介
&#8195;&#8195;作为结构较为复杂的算法之一，神经网络模型在解决很多问题中有着良好的表现。经过长久的发展，时下很多热门的应用模型（如卷积网络，深度学习等）均基于神经网络的基础理论演化而来。因此对神经网络的学习和研究，是机器学习的重要内容之一。  
&#8195;&#8195;所谓神经网络，最早基于生物学，对人脑的神经元的组织形式进行抽象，形成相应的模型，通过不同的组织形式形成的网络模型。神经网络模型中，不同的神经元之间存在着传导的联系，从信息的输入直到最终结果的输出。  
&#8195;&#8195;一个神经网络通常包含着三个基本层级：输入层，隐藏层和输出层。每个层级中会包含若干个神经元，可以认为，一个神经元对应和处理一个维度的变量信息，信息从输入层进入模型，中间经过若干个隐藏层的处理和传递，最终在输出层输出结果。神经网络根据内部结构的差异，可以分为全连接神经网络、卷积神经网络、循环神经网络、深度神经网络（即深度学习）等类型。  
&#8195;&#8195;神经网络模型的优点在于：①非线性映射能力：经过证明，包含隐藏层的三层神经网络可以无限逼近任何非线性连续函数，拟合能力强；②自学习和自适应性：神经网络模型可以自我学习数据间的“规律”，并且将记忆在网络的权值中；③容错性：神经网络各个层级中，即使有部分神经元受到破坏，整个网络已就可以维持运行，具有容错性；而神经网络的局限性在于：①局部极小值：网络过于复杂时，尤其是全连接的组织形式，容易是的模型收敛于局部极值；②收敛速度慢：神经网络基于梯度下降算法，且目标函数通常较为复杂，在接近目标的一些平坦区域时，容易造成权值误差改变很小，训练接近停滞，影响效率。③过拟合：传统的神经网络模型因为其复杂的结构，容易陷入过拟合的情况（可通过正则化等方式缓解）  
&#8195;&#8195;本文介绍的是神经网络的一种较为基本的形式，全连接神经网络。全连接神经网络的特点在于，相邻的两个层级的神经元之间一定存在联系，而同一个层级内的神经元没有彼此的联系，即第i层包含m个神经元，i+1层包含n个神经元，这两个层级之间的共有m*n条连线（权值）。本文所推到并且实现的是单隐层全连接神经网络，除了输入层和输出层以外，中间仅包含一个隐藏层。
![image](https://note.youdao.com/yws/api/personal/file/WEBc2cd94d0a54f71386e1daf01348b5a25?method=download&shareKey=5f4f763f4d17da91a1a620854c5eee67!)
# 单隐层神经网络推导
## 前向传播
&#8195;&#8195;首先来介绍神经网络模型的工作原理，所谓前向传播，指的是数据从输入层输入，从前往后经过内部各个层级，最终从输出层得到结果的过程。
假设输入层输入的数据是一个矩阵X，X包含N行（记录数）和n列（字段数），输入层到隐藏层的权值矩阵为K，K为一个n×h的举证，那么隐藏层H就有h个神经元，由输入层的X点乘K，经过激活函数f得到了隐藏层H：

```math
X=\begin{vmatrix}
x_{11} & x_{12} & x_{13} & ... & x_{1n}\\
x_{21} & x_{22} & x_{23} & ... & x_{2n}\\
... & & & &...\\
x_{N1} & x_{N2} & x_{N3} & ... & x_{Nn}
\end{vmatrix}
K=\begin{vmatrix}
k_{11} & k_{12} & ... & k_{1h}\\
x_{21} & x_{22} & ... & x_{2h}\\
... & & & ...\\
k_{n1} & k_{n2} & ... & k_{nh}
\end{vmatrix}

H=f(X \cdot K)
```
&#8195;&#8195;从隐藏层到输出层的传导也是类似的，假设输出层有是一个m分类的情况，那么隐藏层到输出层的权值W则是一个h×m的矩阵，定义输出层激活函数为F，那么输出层O则是H点乘W经过激活函数F的结果：
```math
H=\begin{vmatrix}
h_{11} & h_{12} & ... & h_{1h}\\
h_{21} & h_{22} & ... & h_{2h}\\
... & & & ...\\
h_{N1} & h_{N2} & ... & h_{Nh}
\end{vmatrix}
W=\begin{vmatrix}
w_{11} & w_{12} & ... & w_{1m}\\
w_{21} & w_{22} & ... & w_{2m}\\
... & & & ...\\
w_{h1} & w_{h2} & ... & w_{hm}
\end{vmatrix}

O=F(H \cdot W)
```
&#8195;&#8195;到此，单隐层网络的一次前向传播就结束了。为了在下节便于介绍，作如下定义：
```math
Z1 = X \cdot K 

H = f(Z1)

Z2 = H \cdot W

O = F(Z2)
```
## 误差反向传播
&#8195;&#8195;为了求解神经网络中各个层级之间的权值，我们采用的是误差反向传播算法（即BP算法）。该方法从误差函数开始，通过链式法则向前求导，得到各个层级的权值梯度，便于梯度下降等其他方法的求解。  
&#8195;&#8195;在单隐层神经网络中，我们首先定义误差损失函数loss,该函数应该基于输出层结果O和真实结果矩阵Y的运算，假设loss（O）= L（O）。为了计算loss结果在权值W和K的梯度，需要逐步从后往前一层一层来计算相应对象的导数：

```math
\frac{\partial loss}{\partial W}=\frac{\partial loss}{\partial O} \frac{\partial O}{\partial Z2}\frac{\partial Z2}{\partial W} = H^T \cdot [L'(O) \times F'(Z2)]

\frac{\partial loss}{\partial K} = \frac{\partial loss}{\partial O}\frac{\partial O}{\partial Z2}\frac{\partial Z2}{\partial H}\frac{\partial H}{\partial Z1}\frac{\partial Z1}{\partial K} = X^T \cdot [([L'(O) \times F'(Z2)]\cdot W^T) \times f'(Z1) ]

```
## 以softmax为输出函数的推导
&#8195;&#8195;为了更加形象地给出单隐层全连接神经网络权值更新的形式，我们定义隐藏层激活函数f(x)为sigmoid函数，而输出层的激活函数F(x)则为softmax多分类函数。下面给出softmax的基本形式,对于有m个类别的x：
```math
F(x_i) = softmax(x_i) = \frac{e^{x_i}}{\sum\limits_{j=1}^m{e^{x_j}}}
```
&#8195;&#8195;接下来，给出softmax函数的导数：
```math
\frac{\partial F(x_i)}{\partial x_k} = \left\{\begin{array}{cc} 
		\frac{e^{x_i}\sum\limits_{j=1}^m{e^{x_j}}-{e^{x_i}}^2}{(\sum\limits_{j=1}^m{e^{x_j}})^2}=F(x_i)(1-F(x_i)), & i = k\\ 
		\frac{-e^{x_i}e^{x_k}}{(\sum\limits_{j=1}^m{e^{x_j}})^2} = -F(x_i)F(x_k), & i \neq k 
	\end{array}\right.
```
&#8195;&#8195;定义，多分类中损失函数loss为交叉熵,给出交叉熵的形式和导数：
```math
loss(Y^{pre}) = \sum\limits_{j=1}^m y_jln(y_{j}^{pre})

\frac{\partial loss(Y^{pre})}{\partial y_i^{pre}} = \frac{y_i}{y_i^{pre}} = \frac{y_i}{F(x_i)}
```
&#8195;&#8195;若是求解loss函数对于入参x的导数，结合链式法则，计算流程如下:

```math
\frac{\partial loss(Y^{pre})}{\partial x_k} = \frac{\partial loss(Y^{pre})}{\partial F(x_k)}\frac{\partial F(x_k)}{\partial x_k}  = \sum\limits_{j=1}^m(\frac{y_j}{F(x_j)}\frac{\partial F(x_k)}{\partial x_k}) 

= \sum\limits_{j \neq k}^m[\frac{y_j}{F(x_j)} \times -F(x_k)F(x_j) ] + \frac{y_k}{F(x_k)} \times F(x_k)(1-F(x_k)) = y_k - F(x_k)\sum\limits_{j=1}^my_j
```
&#8195;&#8195;由于多分类问题中，对于一个个体只有可能属于m中的一个类型，即一行y只能有一个非零值，因此交叉熵loss对于入参x的最终导数为：
```math
\frac{\partial loss(Y^{pre})}{\partial x_k} = y_k-F(x_k)
```
&#8195;&#8195;因此基于多分类的softmax函数为输出层激活函数的反向传播权值的梯度求解以及如下：
```math
F(x) = softmax(x)

f(x) = sigmoid(x)

\frac{\partial loss}{\partial W}=\frac{\partial loss}{\partial O} \frac{\partial O}{\partial Z2}\frac{\partial Z2}{\partial W} = H^T \cdot [L'(O) \times F'(Z2)]
= H^T \cdot [Y-F(Z2)] 

= H^T \cdot (Y-O)

\frac{\partial loss}{\partial K} = \frac{\partial loss}{\partial O}\frac{\partial O}{\partial Z2}\frac{\partial Z2}{\partial H}\frac{\partial H}{\partial Z1}\frac{\partial Z1}{\partial K} = X^T \cdot [([L'(O) \times F'(Z2)]\cdot W^T) \times f'(Z1) ]

=X^T \cdot ([(Y-F(Z2)) \cdot W^T] \times [f(Z1) \times (1-f(Z1))])

=X^T \cdot ([(Y-O) \cdot W^T] \times [H \times (1-H)])

W_{t+1} = W_t - \frac{\partial loss}{\partial W} \times step

K_{t+1} = K_t - \frac{\partial loss}{\partial K} \times step
```
## 神经网络流程
&#8195;&#8195;在此，我们给出全连接神经网络的训练流程:  
① 初始化权值：包含了从输入层X到隐藏层H的权值K，从隐藏层H到输出层O的权值W  
② 迭代流程开始  
③ 前向传播：从X开始，经过权值和激活函数的运算，最终得到输出层结果O  
④ 反向传播：从误差函数开始，向前求解各个层之间权值的梯度  
⑤ 权值更新：采用梯度下降更新权值  
⑥ 重新开始第③步，直到满足停止条件

# 简单神经网络的实现
## 代码介绍
&#8195;&#8195;我们首先加载相应模块，定义一个神经网络类，与此同时定义所需的类内函数，激活函数采用sigmoid函数，并定义相应的导数，当面临多分类输出时，将会用到softmax多分类输出函数。
```python
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_breast_cancer #乳腺癌数据加载
from sklearn.cross_validation import train_test_split
class JQ_nnet():
    import numpy as np
    def sigmoid(self,x): #激活函数（sigmoid函数）
        return (1/(1+np.exp(-x)))
    def sigmoid_der(self,x): #sigmoid函数的导数
        return self.sigmoid(x)*(1-self.sigmoid(x))    
    def softmax(self,x): #多分类输出函数（softmax函数）
        return np.apply_along_axis(lambda x: np.exp(x)*1.0/np.exp(x).sum(),1,x)
```
&#8195;&#8195;接下来定义主体模型的训练函数。入参包含了训练自变量和标签。另外还有隐藏层神经元数量，迭代次数，步长等参数。模型的训练首先初始化输入层到隐藏层，隐藏层到输出层的参数，在每一次迭代中，首先根据当前权值参数从输入层开始前向传播，输出结果。然后进行反向传播，利用误差求解权值的梯度，更新权值。
```python
    def fit(self,x,y,
            hidden = 5, #隐藏层数量
            iter=5000, #迭代次数
            s=0.01, #步长
            decay = 0.001, #步长衰减因子
            batch_size = 100, #每次梯度下降抽取的样本量
            loss='binary_crossentropy', #分类损失函数（可选二分类binary_crossentropy和多分类crossentropy）
            error_iter=1000, #每迭代多少次打印当前loss值
            seed=10): #随机种子
        x = np.matrix(np.hstack((np.ones((x.shape[0],1)),x)))
        try:
            y.shape[1]
            y = np.matrix(y)
        except:
            y = np.matrix(y).T
        length1 = x.shape[1]        
        width1 = hidden
        length2 = hidden + 1
        width2 = y.shape[1]
        s_start = s
        np.random.seed(seed)
        K = np.random.rand(length1,width1)*2-1 #初始化参数：从输入层到隐含层
        np.random.seed(seed+100) #随机种子固定
        W = np.random.rand(length2,width2)*2-1 #初始化参数：从隐含层到输出层
        i = 1
        while i <= iter: #迭代过程
            np.random.seed(seed+i*10) #随机种子固定
            sample_idx = np.random.choice(x.shape[0],batch_size,replace=False) #随机梯度下降采样
            x_s = x[sample_idx,:] #输入层采样
            y_s = y[sample_idx,:] #标签采样
            if i % error_iter != 0:
                Z1_s = x_s.dot(K) #隐藏层计算（输入层*权值）
                H_s = np.hstack((np.ones((x_s.shape[0],1)),self.sigmoid(Z1_s))) #隐藏层计算（激活函数）
                Z2_s = H_s.dot(W) #输出层计算（隐藏层*权值）
                if loss == 'binary_crossentropy': #二分类问题流程
                    y_pre_s = self.sigmoid(Z2_s) #输出层计算（激活函数：sigmoid）
                else:
                    y_pre_s = self.softmax(Z2_s) #输出层计算（激活函数：softmax）
            else:
                Z1 = x.dot(K) #隐藏层计算（输入层*权值）
                H = np.hstack((np.ones((x.shape[0],1)),self.sigmoid(Z1))) #隐藏层计算（激活函数）
                Z2 = H.dot(W) #输出层计算（隐藏层*权值）
                H_s = H[sample_idx,:] #隐藏层采样
                x_s = x[sample_idx,:] #输入层采样
                Z1_s = Z1[sample_idx,:] #隐藏层采样
                if loss == 'binary_crossentropy': #二分类问题流程
                    y_pre = self.sigmoid(Z2) 
                    error = -(np.array(y) * np.log(np.array(y_pre))+(1-np.array(y))*np.log(1-np.array(y_pre))).sum()
                    print 'n_iter: ', i, ' binary_crossentropy: ', error
                else:
                    y_pre = self.softmax(Z2) 
                    error = -(np.array(np.log(y_pre))*np.array(y)).sum()
                    print 'n_iter: ', i, ' crossentropy: ', error
                y_pre_s = y_pre[sample_idx,:] #输出层采样
            der_Z1 = np.matrix(self.sigmoid_der(np.array(Z1_s)))
            gra_W = H_s.T.dot(np.matrix(np.array(y_pre_s-y_s)))/float(batch_size)#权值梯度计算（隐藏层——输出层）
            gra_K = x_s.T.dot(np.array(np.matrix(y_pre_s-y_s).dot(W[1:,:].T)) * np.array(der_Z1))/float(batch_size) #权值梯度计算（输入层——隐藏层）
            W = W - gra_W * s #权值更新（隐藏层——输出层）
            K = K - gra_K * s #权值更新（输入层——隐藏层）
            s = s_start / (1+i*decay) #步长衰减
            i += 1
        self.W = W
        self.K = K    
```
&#8195;&#8195;模型的迭代优化是基于随机批处理梯度下降进行的，每一次迭代只从总体中抽取一定数量的样本来用于权值更新。通过调节参数loss，模型可同时支持二分类和多分类问题。需要特别说明的是，权值更新的步长s并不是固定不变的，会结合decay参数进行衰减，随着迭代次数增多，越来越接近目标，步长也会相应缩小。这样可以缓解步长过长而引发震荡影响收敛的问题。
```math
s_{n}=\frac{s_0}{1+n*decay}
```
&#8195;&#8195;之后，我们定义模型的预测函数。包含了概率预测和类别预测。
```python
    def predict_prob(self,x): #预测函数（概率预测）
        x = np.matrix(np.hstack((np.ones((x.shape[0],1)),x)))
        Z1 = x.dot(self.K)
        H = np.hstack((np.ones((x.shape[0],1)),self.sigmoid(Z1)))
        Z2 = H.dot(self.W)
        if self.W.shape[1]>1:
            res = np.array(self.softmax(Z2))
        else:
            res = np.array(self.sigmoid(Z2)).T[0]
        return res
    def predict_type(self,x): #预测函数（类别预测）
        res_prob = self.predict_prob(x)   
        try:
            res_prob.shape[1]
            res = np.apply_along_axis(lambda x: [int(i) for i in x/x.max()],1,res_prob)
        except:
            res = np.array(np.round(res_prob))
        return (res)
```
&#8195;&#8195;最后，我们定义模型存储和加载函数，基于这些函数我们可以将训练结果保存，以便未来读取使用。
```python
    def save_model(self,path): #模型参数存储函数
        model_res = {}
        model_res['W'] = self.W
        model_res['K'] = self.K
        output = open(path+'.pkl', 'wb')
        pickle.dump(model_res, output)
        output.close()
    def load_model(self,path): #模型参数读取函数
        model_file = open(path+'.pkl', 'rb')
        model_res = pickle.load(model_file) 
        self.K = model_res['K']
        self.W = model_res['W']
        model_file.close()
```
## 乳腺癌数据集二分类的尝试
### 数据加载
&#8195;&#8195;对于实现的全连接神经网络，先解决一个简单的二分类问题，乳腺癌的区分，利用sklearn模块加载数据集。数据集包含了569条记录，30个字段，而目标target的1对应了患乳腺癌的情况，0则对应未患此病，划分训练集和测试集，其中测试集占比20%。
```python
#读取乳腺癌数据
cancer_data = load_breast_cancer()['data']
cancer_target = load_breast_cancer()['target']
cancer_data_train,cancer_data_test,cancer_target_train,cancer_target_test = train_test_split(cancer_data,cancer_target,test_size=0.2)
```
### 建模尝试
&#8195;&#8195;运用模型进行建模尝试，设置隐藏层神经元数量为20，迭代次数为30000次，初始步长0.005，每隔1000次迭代打印一次当前误差。训练完成以后，进行预测并且查看准确度结果。
```python
#乳腺癌测试数据二分类尝试
cancer_model = JQ_nnet() #实例化
cancer_model.fit(cancer_data_train,
                 cancer_target_train,
                 hidden=20,
                 iter=30000,
                 s=0.005,
                 error_iter=1000) #训练模型，迭代30000次，搜索步长为0.005
ann_pre_tr = cancer_model.predict_type(cancer_data_train) #训练集预测
ann_pre_te = cancer_model.predict_type(cancer_data_test) #测试集预测
accuracy_score(cancer_target_train,ann_pre_tr) #准确度：训练集
accuracy_score(cancer_target_test,ann_pre_te) #准确度：测试集
```
&#8195;&#8195;可以看出，在模型的迭代过程中，误差显著下降了。  
![image](https://note.youdao.com/yws/api/personal/file/WEBb591ce1b075911e94af2b576d88abdca?method=download&shareKey=49c6589b99bf5bdb80f9fd589c3afa76)   
&#8195;&#8195;模型对训练集和测试集的预测均能够达到0.92的水平，效果不错。
![image](https://note.youdao.com/yws/api/personal/file/WEB4d8e013d793e03f0ef18bf52a15c6d3a?method=download&shareKey=cc7ed7738c295f30ded3acd2dbc809ff) 
## Mnist手写数字识别多分类的尝试
### 数据集的读取和转换
&#8195;&#8195;接下来，我们会尝试一个相对复杂的问题，手写数字分类，采用的数据集是著名的Mnist数据集，该数据集包含了训练集和测试集两个部分，训练集包含了60000个样本，测试集包含了10000个样本。其中，每个个体是一张28×28的灰度图片，对应数字0-9的10个类别。首先我们读取数据集：
```python
#读取mnist
import gzip #使用zlib来压缩和解压缩数据文件，读写gzip文件
import struct #通过引入struct模块来处理图片中的二进制数据

def read_data(label_path,image_path): #定义读取数据的函数
    with gzip.open(label_path) as flbl: #解压标签包
        magic, num = struct.unpack(">II",flbl.read(8)) #采用Big Endian的方式读取两个int类型的数据，且参考MNIST官方格式介绍，magic即为magic number (MSB first) 用于表示文件格式，num即为文件夹内包含的数据的数量
        label = np.fromstring(flbl.read(),dtype=np.int8) #将标签包中的每一个二进制数据转化成其对应的十进制数据，且转换后的数据格式为int8（-128 to 127）格式，返回一个数组
    with gzip.open(image_path,'rb') as fimg: #已只读形式解压图像包
        magic, num, rows, cols = struct.unpack(">IIII",fimg.read(16)) #采用Big Endian的方式读取四个int类型数据，且参考MNIST官方格式介绍，magic和num上同，rows和cols即表示图片的行数和列数
        image = np.fromstring(fimg.read(),dtype=np.uint8).reshape(len(label),rows,cols) #将图片包中的二进制数据读取后转换成无符号的int8格式的数组，并且以标签总个数，行数，列数重塑成一个新的多维数组
    return (label,image) #返回读取成功的label数组和image数组

(train_lbl, train_img) = read_data('train-labels-idx1-ubyte.gz','train-images-idx3-ubyte.gz') #构建训练数据
(val_lbl, val_img) = read_data('t10k-labels-idx1-ubyte.gz','t10k-images-idx3-ubyte.gz') #构建测试数据
```
&#8195;&#8195;由于训练数据集较大，作为示例，我们将从中随机抽取10%作为训练数据，同时，我们要将样本从N×28×28的数组展平为N×784的矩阵形式。对于标签，在模型训练之前，需要对标签转化为独热编码的矩阵。然后分别得到用于训练和测试的X和Y。
```python
def label_tsf(x): #多分类标签编码函数
    res = np.zeros([x.shape[0],np.unique(x).shape[0]])
    for i in range(x.shape[0]):
        res[i,x[i]]=1
    return res

np.random.seed(10) #随机种子
sample_idx =np.random.choice(60000,6000,replace=False) #随机抽取6000个样本
mnist_train_label = train_lbl[sample_idx]
mnist_train_y = label_tsf(mnist_train_label)
mnist_train_x = train_img[sample_idx,:,:].reshape([6000,784])
mnist_test_x = val_img.reshape([10000,784])
mnist_test_y = label_tsf(val_lbl)
del train_lbl, train_img, val_lbl, val_img
```
### 建模尝试
&#8195;&#8195;对于Mnist数据集的神经网络建模尝试，我们采用300个隐藏层神经元，20000次迭代，初始补偿设置为0.1。建模完成后对10000个测试集进行预测。
```python
#MNIST数据集多分类尝试
ann_model = JQ_nnet() #实例化
ann_model.fit(mnist_train_x,
              mnist_train_y,
              hidden=300, #隐层神经元
              iter=20000, #迭代数
              s=0.1, #初始步长
              decay=0.001, #衰减指数
              batch_size=100, #批处理数量
              loss='crossentropy', #多分类交叉熵损失函数
              error_iter=1000) #训练模型
ann_pre = ann_model.predict_prob(mnist_test_x)
accuracy_score(mnist_train_y,ann_model.predict_type(mnist_train_x)) #准确度：训练集
accuracy_score(mnist_test_y,ann_model.predict_type(mnist_test_x))#准确度：测试集
ann_model.save_model('nnet_model_mnist') #模型存储
ann_model.load_model('nnet_model_mnist') #模型加载
```
&#8195;&#8195;我们对结果进行观察，发现模型在训练集上的准确度表现超过了93%，在测试集上面的表现也达到了85%，实际上对参数进行调整并且采用一些预防过拟合的措施，可以进一步提升预测效果。
![image](https://note.youdao.com/yws/api/personal/file/WEB41493a0feb47e998b1b46d88b91e220f?method=download&shareKey=e0cae34231c8f8f2e338224becd1791f) 
# 拓展：keras模块实现简单神经网络建模
## Keras简介
&#8195;&#8195;Keras模块是python可以用于神经网络模型训练的模块之一，相比于目前主流的库，更容易理解和上手，和常规的Python代码有所差异，Keras建模类似于一种“管道操作”，逐步将模型搭建完成，形成模型的“壳”，之后在进行整个模型的数据训练。
Keras底层是基于Theano或TensorFlow计算库。本文在此不会介绍Keras的具体安装，建议采用linux或者OSX等系统环境下的Keras来进行下列示例的尝试。
## Keras在Mnist手写数据识别的应用
&#8195;&#8195;首先加载相关的模块和数据，本例建模采用的是序贯模型，即多个网络层形成堆叠，涉及到的全连接层Dense，激活层Activation和Dropout层（防止过拟合）。在训练中，我们将采用mnist训练集全部的60000条记录。
```python
#keras模块神经网络建模
#对于keras模块的安装需要依赖于Theano或者Tensforflow框架，在Windows系统中安装较繁琐，因此下列步骤建议在Linux等系统中尝试
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras import optimizers

train_X = train_img.reshape([60000,784])
train_Y = label_tsf(train_lbl)
test_X = val_img.reshape([10000,784]) 
test_Y = val_lbl
```
&#8195;&#8195;下一步，就要开始构建网络了。以下代码构建了单隐层全连接神经网络，从输入层开始，首先dropout20%的神经元（这些神经元指定为0），之后进入隐藏层，激活函数为sigmoid。通用dropout隐藏层的20%的神经元，进入输出层，激活函数为softma：
```python
net = Sequential() #实例化
net.add(Dropout(0.2,input_shape=(784,))) #过滤掉输入层20%的神经元
net.add(Dense(300,input_dim=784)) #输入层到隐藏层设置，输入层神经元为X数量，隐藏层神经元数10
net.add(Activation("sigmoid")) #输入层到隐藏层函数为sigmoid
net.add(Dropout(0.2)) #过滤掉20%的神经元
net.add(Dense(10,input_dim=300)) #隐藏层到输出层设置，上一隐藏层神经元为300，输出层神经元为10
net.add(Activation("softmax")) #隐藏层到输出层函数为softmax
```
&#8195;&#8195;在开始正式训练模型之前，需要对模型进行编译：首先设置方法为随机梯度下降，步长0.01，衰减指数0.000001。损失函数为多分类交叉熵，优化目标为准确度。开始模型的训练，迭代100次，批处理记录数100。
```python
sgd = optimizers.SGD(lr=0.01, decay=1e-6) #训练方法为随机梯度下降SGD
net.compile(loss='categorical_crossentropy', #损失函数binary_crossentropy
            optimizer=sgd,
            metrics=['accuracy']) #模型编译，目标为准确度
net.fit(train_X,train_Y,epochs=100,batch_size=100) #模型训练,设置所有数据的迭代次数为100次，批处理记录数100，实际上每一次迭代以100为单位，将全部的60000条数据依次尝试，全过程总共60000×100=6000000条样本被尝试。
```
&#8195;&#8195;训练过程需要耗费一点时间，完成之后即可对数据进行预测。
![image](https://note.youdao.com/yws/api/personal/file/WEB5bd0b6330e7ef02a849cdb260cec8930?method=download&shareKey=147607cec75c11a6493f1cf15fc1e122)   
```python
net_predict = net.predict_classes(test_X) #预测类别
accuracy_score(train_lbl,net.predict_classes(train_X)) #预测准确度
accuracy_score(val_lbl,net_predict) #预测准确度
```
&#8195;&#8195;结果显示，基于Keras和mnist全量数据训练的模型在训练集和测试集上分别达到了98%和97%的准确度，这已经接近了全连接神经网络在该数据集可以实现的最好效果。实际上，如果采用其他形式的神经网络模型，如卷积神经网络，可以实现更高的准确度的预测，该模型的实现将在后续文章中进行介绍。
![image](https://note.youdao.com/yws/api/personal/file/WEBcc4d56f0f165b15a0fa79eb1da842005?method=download&shareKey=19036edbb18bd3ad51e15b12866b08aa)   


